{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1NzP4oI_KighbpfEEVCnYKZ0lfzDpqTyE","timestamp":1714564389326},{"file_id":"11z58bl3meSzo6kFqkahMa35G5jmh2Wgt","timestamp":1605160735857},{"file_id":"1GFSsqP2BWz4gtq0e-nki00ZHSirXwFyY","timestamp":1592100981251},{"file_id":"1fgRNZxFag-YyLOhVke77Non19YiZ6raM","timestamp":1586659114015},{"file_id":"1Z2_uKAAvOtQVrUulcxmP6m9TD-Pegl0r","timestamp":1586656948105},{"file_id":"1MEfpnXIxxbw2tjSRc1hqkxb2HrTToTBn","timestamp":1586052797157}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"eclLG4xlJRIE"},"source":["<h3>Goal: Image to 3D model (PIFuHD)</h3>     <a href = \"https://shunsukesaito.github.io/PIFuHD/ \">Project Page</a>    |    <a href = \"https://github.com/facebookresearch/pifuhd\">Github</a>    |    <a href = \"https://arxiv.org/pdf/2004.00452.pdf\">Paper</a>    |    <a href = \"https://colab.research.google.com/drive/11z58bl3meSzo6kFqkahMa35G5jmh2Wgt\">Original Notebook</a>\n","\n","<img src = \"https://shunsukesaito.github.io/PIFuHD/resources/images/pifuhd.gif\" height = \"400\">\n","\n","<small>Original Notebook Made by <small>[![Follow](https://img.shields.io/twitter/follow/psyth91?style=social)](https://twitter.com/psyth91)"]},{"cell_type":"code","metadata":{"id":"Bbzauji1E2tm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714564480632,"user_tz":-480,"elapsed":30612,"user":{"displayName":"H Y","userId":"00495941265729836346"}},"outputId":"a9f0d8d3-9ef9-41d0-e047-e4be2063e73e"},"source":["#@title STEP1: Execute to Setup Pifuhd\n","!git clone https://github.com/facebookresearch/pifuhd\n","!git clone https://github.com/Daniil-Osokin/lightweight-human-pose-estimation.pytorch.git\n","\n","%cd /content/lightweight-human-pose-estimation.pytorch/\n","!wget https://download.01.org/opencv/openvino_training_extensions/models/human_pose_estimation/checkpoint_iter_370000.pth\n","\n","%cd /content/pifuhd/\n","!sh ./scripts/download_trained_model.sh\n","\n","\n","!pip install 'torch==1.6.0+cu101' -f https://download.pytorch.org/whl/torch_stable.html\n","!pip install 'torchvision==0.7.0+cu101' -f https://download.pytorch.org/whl/torch_stable.html\n","!pip install 'pytorch3d==0.2.5'\n","\n","\n","%cd /content/lightweight-human-pose-estimation.pytorch/\n","import torch\n","import cv2\n","import numpy as np\n","from models.with_mobilenet import PoseEstimationWithMobileNet\n","from modules.keypoints import extract_keypoints, group_keypoints\n","from modules.load_state import load_state\n","from modules.pose import Pose, track_poses\n","import demo\n","from IPython.display import clear_output\n","\n","def get_rect(net, images, height_size):\n","    net = net.eval()\n","\n","    stride = 8\n","    upsample_ratio = 4\n","    num_keypoints = Pose.num_kpts\n","    previous_poses = []\n","    delay = 33\n","    for image in images:\n","        rect_path = image.replace('.%s' % (image.split('.')[-1]), '_rect.txt')\n","        img = cv2.imread(image, cv2.IMREAD_COLOR)\n","        orig_img = img.copy()\n","        orig_img = img.copy()\n","        heatmaps, pafs, scale, pad = demo.infer_fast(net, img, height_size, stride, upsample_ratio, cpu=False)\n","\n","        total_keypoints_num = 0\n","        all_keypoints_by_type = []\n","        for kpt_idx in range(num_keypoints):  # 19th for bg\n","            total_keypoints_num += extract_keypoints(heatmaps[:, :, kpt_idx], all_keypoints_by_type, total_keypoints_num)\n","\n","        pose_entries, all_keypoints = group_keypoints(all_keypoints_by_type, pafs)\n","        for kpt_id in range(all_keypoints.shape[0]):\n","            all_keypoints[kpt_id, 0] = (all_keypoints[kpt_id, 0] * stride / upsample_ratio - pad[1]) / scale\n","            all_keypoints[kpt_id, 1] = (all_keypoints[kpt_id, 1] * stride / upsample_ratio - pad[0]) / scale\n","        current_poses = []\n","\n","        rects = []\n","        for n in range(len(pose_entries)):\n","            if len(pose_entries[n]) == 0:\n","                continue\n","            pose_keypoints = np.ones((num_keypoints, 2), dtype=np.int32) * -1\n","            valid_keypoints = []\n","            for kpt_id in range(num_keypoints):\n","                if pose_entries[n][kpt_id] != -1.0:  # keypoint was found\n","                    pose_keypoints[kpt_id, 0] = int(all_keypoints[int(pose_entries[n][kpt_id]), 0])\n","                    pose_keypoints[kpt_id, 1] = int(all_keypoints[int(pose_entries[n][kpt_id]), 1])\n","                    valid_keypoints.append([pose_keypoints[kpt_id, 0], pose_keypoints[kpt_id, 1]])\n","            valid_keypoints = np.array(valid_keypoints)\n","\n","            if pose_entries[n][10] != -1.0 or pose_entries[n][13] != -1.0:\n","              pmin = valid_keypoints.min(0)\n","              pmax = valid_keypoints.max(0)\n","\n","              center = (0.5 * (pmax[:2] + pmin[:2])).astype(np.int)\n","              radius = int(0.65 * max(pmax[0]-pmin[0], pmax[1]-pmin[1]))\n","            elif pose_entries[n][10] == -1.0 and pose_entries[n][13] == -1.0 and pose_entries[n][8] != -1.0 and pose_entries[n][11] != -1.0:\n","              # if leg is missing, use pelvis to get cropping\n","              center = (0.5 * (pose_keypoints[8] + pose_keypoints[11])).astype(np.int)\n","              radius = int(1.45*np.sqrt(((center[None,:] - valid_keypoints)**2).sum(1)).max(0))\n","              center[1] += int(0.05*radius)\n","            else:\n","              center = np.array([img.shape[1]//2,img.shape[0]//2])\n","              radius = max(img.shape[1]//2,img.shape[0]//2)\n","\n","            x1 = center[0] - radius\n","            y1 = center[1] - radius\n","\n","            rects.append([x1, y1, 2*radius, 2*radius])\n","\n","        np.savetxt(rect_path, np.array(rects), fmt='%d')\n"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'pifuhd'...\n","remote: Enumerating objects: 222, done.\u001b[K\n","remote: Counting objects: 100% (126/126), done.\u001b[K\n","remote: Compressing objects: 100% (44/44), done.\u001b[K\n","remote: Total 222 (delta 92), reused 82 (delta 82), pack-reused 96\u001b[K\n","Receiving objects: 100% (222/222), 399.39 KiB | 2.11 MiB/s, done.\n","Resolving deltas: 100% (114/114), done.\n","Cloning into 'lightweight-human-pose-estimation.pytorch'...\n","remote: Enumerating objects: 124, done.\u001b[K\n","remote: Counting objects: 100% (34/34), done.\u001b[K\n","remote: Compressing objects: 100% (15/15), done.\u001b[K\n","remote: Total 124 (delta 21), reused 19 (delta 19), pack-reused 90\u001b[K\n","Receiving objects: 100% (124/124), 230.30 KiB | 999.00 KiB/s, done.\n","Resolving deltas: 100% (53/53), done.\n","/content/lightweight-human-pose-estimation.pytorch\n","--2024-05-01 11:54:15--  https://download.01.org/opencv/openvino_training_extensions/models/human_pose_estimation/checkpoint_iter_370000.pth\n","Resolving download.01.org (download.01.org)... 23.55.198.252, 2600:1406:6c00:19b::a87, 2600:1406:6c00:1a7::a87\n","Connecting to download.01.org (download.01.org)|23.55.198.252|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 87959810 (84M) [application/octet-stream]\n","Saving to: ‘checkpoint_iter_370000.pth’\n","\n","checkpoint_iter_370 100%[===================>]  83.88M   150MB/s    in 0.6s    \n","\n","2024-05-01 11:54:16 (150 MB/s) - ‘checkpoint_iter_370000.pth’ saved [87959810/87959810]\n","\n","/content/pifuhd\n","+ mkdir -p checkpoints\n","+ cd checkpoints\n","+ wget https://dl.fbaipublicfiles.com/pifuhd/checkpoints/pifuhd.pt pifuhd.pt\n","--2024-05-01 11:54:17--  https://dl.fbaipublicfiles.com/pifuhd/checkpoints/pifuhd.pt\n","Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 13.226.210.111, 13.226.210.25, 13.226.210.78, ...\n","Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|13.226.210.111|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1548375177 (1.4G) [application/octet-stream]\n","Saving to: ‘pifuhd.pt’\n","\n","pifuhd.pt           100%[===================>]   1.44G   114MB/s    in 13s     \n","\n","2024-05-01 11:54:30 (110 MB/s) - ‘pifuhd.pt’ saved [1548375177/1548375177]\n","\n","--2024-05-01 11:54:30--  http://pifuhd.pt/\n","Resolving pifuhd.pt (pifuhd.pt)... failed: Name or service not known.\n","wget: unable to resolve host address ‘pifuhd.pt’\n","FINISHED --2024-05-01 11:54:30--\n","Total wall clock time: 14s\n","Downloaded: 1 files, 1.4G in 13s (110 MB/s)\n","Looking in links: https://download.pytorch.org/whl/torch_stable.html\n","\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.6.0+cu101 (from versions: 1.11.0, 1.11.0+cpu, 1.11.0+cu102, 1.11.0+cu113, 1.11.0+cu115, 1.11.0+rocm4.3.1, 1.11.0+rocm4.5.2, 1.12.0, 1.12.0+cpu, 1.12.0+cu102, 1.12.0+cu113, 1.12.0+cu116, 1.12.0+rocm5.0, 1.12.0+rocm5.1.1, 1.12.1, 1.12.1+cpu, 1.12.1+cu102, 1.12.1+cu113, 1.12.1+cu116, 1.12.1+rocm5.0, 1.12.1+rocm5.1.1, 1.13.0, 1.13.0+cpu, 1.13.0+cu116, 1.13.0+cu117, 1.13.0+cu117.with.pypi.cudnn, 1.13.0+rocm5.1.1, 1.13.0+rocm5.2, 1.13.1, 1.13.1+cpu, 1.13.1+cu116, 1.13.1+cu117, 1.13.1+cu117.with.pypi.cudnn, 1.13.1+rocm5.1.1, 1.13.1+rocm5.2, 2.0.0, 2.0.0+cpu, 2.0.0+cpu.cxx11.abi, 2.0.0+cu117, 2.0.0+cu117.with.pypi.cudnn, 2.0.0+cu118, 2.0.0+rocm5.3, 2.0.0+rocm5.4.2, 2.0.1, 2.0.1+cpu, 2.0.1+cpu.cxx11.abi, 2.0.1+cu117, 2.0.1+cu117.with.pypi.cudnn, 2.0.1+cu118, 2.0.1+rocm5.3, 2.0.1+rocm5.4.2, 2.1.0, 2.1.0+cpu, 2.1.0+cpu.cxx11.abi, 2.1.0+cu118, 2.1.0+cu121, 2.1.0+cu121.with.pypi.cudnn, 2.1.0+rocm5.5, 2.1.0+rocm5.6, 2.1.1, 2.1.1+cpu, 2.1.1+cpu.cxx11.abi, 2.1.1+cu118, 2.1.1+cu121, 2.1.1+cu121.with.pypi.cudnn, 2.1.1+rocm5.5, 2.1.1+rocm5.6, 2.1.2, 2.1.2+cpu, 2.1.2+cpu.cxx11.abi, 2.1.2+cu118, 2.1.2+cu121, 2.1.2+cu121.with.pypi.cudnn, 2.1.2+rocm5.5, 2.1.2+rocm5.6, 2.2.0, 2.2.0+cpu, 2.2.0+cpu.cxx11.abi, 2.2.0+cu118, 2.2.0+cu121, 2.2.0+rocm5.6, 2.2.0+rocm5.7, 2.2.1, 2.2.1+cpu, 2.2.1+cpu.cxx11.abi, 2.2.1+cu118, 2.2.1+cu121, 2.2.1+rocm5.6, 2.2.1+rocm5.7, 2.2.2, 2.2.2+cpu, 2.2.2+cpu.cxx11.abi, 2.2.2+cu118, 2.2.2+cu121, 2.2.2+rocm5.6, 2.2.2+rocm5.7, 2.3.0, 2.3.0+cpu, 2.3.0+cpu.cxx11.abi, 2.3.0+cu118, 2.3.0+cu121, 2.3.0+rocm5.7, 2.3.0+rocm6.0)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for torch==1.6.0+cu101\u001b[0m\u001b[31m\n","\u001b[0mLooking in links: https://download.pytorch.org/whl/torch_stable.html\n","\u001b[31mERROR: Could not find a version that satisfies the requirement torchvision==0.7.0+cu101 (from versions: 0.1.6, 0.1.7, 0.1.8, 0.1.9, 0.2.0, 0.2.1, 0.2.2, 0.2.2.post2, 0.2.2.post3, 0.12.0, 0.12.0+cpu, 0.12.0+cu102, 0.12.0+cu113, 0.12.0+cu115, 0.12.0+rocm4.3.1, 0.12.0+rocm4.5.2, 0.13.0, 0.13.0+cpu, 0.13.0+cu102, 0.13.0+cu113, 0.13.0+cu116, 0.13.0+rocm5.0, 0.13.0+rocm5.1.1, 0.13.1, 0.13.1+cpu, 0.13.1+cu102, 0.13.1+cu113, 0.13.1+cu116, 0.13.1+rocm5.0, 0.13.1+rocm5.1.1, 0.14.0, 0.14.0+cpu, 0.14.0+cu116, 0.14.0+cu117, 0.14.0+rocm5.1.1, 0.14.0+rocm5.2, 0.14.1, 0.14.1+cpu, 0.14.1+cu116, 0.14.1+cu117, 0.14.1+rocm5.1.1, 0.14.1+rocm5.2, 0.15.0, 0.15.0+cpu, 0.15.0+cu117, 0.15.0+cu118, 0.15.0+rocm5.3, 0.15.0+rocm5.4.2, 0.15.1, 0.15.1+cpu, 0.15.1+cu117, 0.15.1+cu118, 0.15.1+rocm5.3, 0.15.1+rocm5.4.2, 0.15.2, 0.15.2+cpu, 0.15.2+cu117, 0.15.2+cu118, 0.15.2+rocm5.3, 0.15.2+rocm5.4.2, 0.16.0, 0.16.0+cpu, 0.16.0+cu118, 0.16.0+cu121, 0.16.0+rocm5.5, 0.16.0+rocm5.6, 0.16.1, 0.16.1+cpu, 0.16.1+cu118, 0.16.1+cu121, 0.16.1+rocm5.5, 0.16.1+rocm5.6, 0.16.2, 0.16.2+cpu, 0.16.2+cu118, 0.16.2+cu121, 0.16.2+rocm5.5, 0.16.2+rocm5.6, 0.17.0, 0.17.0+cpu, 0.17.0+cu118, 0.17.0+cu121, 0.17.0+rocm5.6, 0.17.0+rocm5.7, 0.17.1, 0.17.1+cpu, 0.17.1+cu118, 0.17.1+cu121, 0.17.1+rocm5.6, 0.17.1+rocm5.7, 0.17.2, 0.17.2+cpu, 0.17.2+cu118, 0.17.2+cu121, 0.17.2+rocm5.6, 0.17.2+rocm5.7, 0.18.0, 0.18.0+cpu, 0.18.0+cu118, 0.18.0+cu121, 0.18.0+rocm5.7, 0.18.0+rocm6.0)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for torchvision==0.7.0+cu101\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement pytorch3d==0.2.5 (from versions: none)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for pytorch3d==0.2.5\u001b[0m\u001b[31m\n","\u001b[0m/content/lightweight-human-pose-estimation.pytorch\n"]}]},{"cell_type":"markdown","metadata":{"id":"YaiNm_QAyx7h"},"source":["# Image from web"]},{"cell_type":"code","metadata":{"id":"8KHPtNWwObj-","colab":{"base_uri":"https://localhost:8080/","height":512},"executionInfo":{"status":"error","timestamp":1714565348370,"user_tz":-480,"elapsed":1815,"user":{"displayName":"H Y","userId":"00495941265729836346"}},"outputId":"8ac794f2-495d-4c48-b68a-61189ceca599"},"source":["#@title STEP2: Testing on web images\n","#@markdown * Find an image on the web\n","#@markdown * Right click and copy image address\n","#@markdown * Paste the image address to image_url\n","image_url = '/content/ekr4qNyxqwwxDzwSzJE3NUKoqwcgOLgzCUP0VR5k.jpeg' #@param {type:\"string\"}\n","path = '/content/ekr4qNyxqwwxDzwSzJE3NUKoqwcgOLgzCUP0VR5k.jpeg'\n","\n","#empty the folder\n","!rm -rf '/content/pifuhd/sample_images'\n","!mkdir '/content/pifuhd/sample_images'\n","\n","!wget '$image_url' -O '$path'\n","import cv2\n","img = cv2.imread(path)\n","cv2.imwrite('/content/ekr4qNyxqwwxDzwSzJE3NUKoqwcgOLgzCUP0VR5k.jpeg', img)\n","!rm -f '$path'\n","\n","import os\n","image_path = '/content/ekr4qNyxqwwxDzwSzJE3NUKoqwcgOLgzCUP0VR5k.jpeg'\n","image_dir = os.path.dirname(image_path)\n","file_name = os.path.splitext(os.path.basename(image_path))[0]\n","\n","# output pathes\n","obj_path = '/content/pifuhd/results/pifuhd_final/recon/result_%s_256.obj' % file_name\n","out_img_path = '/content/pifuhd/results/pifuhd_final/recon/result_%s_256.png' % file_name\n","video_path = '/content/pifuhd/results/pifuhd_final/recon/result_%s_256.mp4' % file_name\n","video_display_path = '/content/pifuhd/results/pifuhd_final/result_%s_256_display.mp4' % file_name\n","\n","#preprosessing\n","%cd /content/lightweight-human-pose-estimation.pytorch/\n","\n","net = PoseEstimationWithMobileNet()\n","checkpoint = torch.load('checkpoint_iter_370000.pth', map_location='cpu')\n","load_state(net, checkpoint)\n","get_rect(net.cuda(), [image_path], 512)\n","\n","\n","#run\n","%cd /content/pifuhd/\n","# Warning: all images with the corresponding rectangle files under -i will be processed.\n","!python -m apps.simple_test -r 256 --use_rect -i $image_dir\n","# seems that 256 is the maximum resolution that can fit into Google Colab.\n","# If you want to reconstruct a higher-resolution mesh, please try with your own machine.\n","\n","#clear everything\n","clear_output()\n","#render video\n","from lib.colab_util import generate_video_from_obj, set_renderer, video\n","\n","renderer = set_renderer()\n","generate_video_from_obj(obj_path, out_img_path, video_path, renderer)\n","\n","# we cannot play a mp4 video generated by cv2\n","!ffmpeg -i $video_path -vcodec libx264 $video_display_path -y -loglevel quiet\n","video(video_display_path)"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["--2024-05-01 12:09:10--  http://$image_url/\n","Resolving $image_url ($image_url)... failed: Name or service not known.\n","wget: unable to resolve host address ‘$image_url’\n","/content/lightweight-human-pose-estimation.pytorch\n"]},{"output_type":"error","ename":"AttributeError","evalue":"module 'numpy' has no attribute 'int'.\n`np.int` was a deprecated alias for the builtin `int`. To avoid this error in existing code, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-e420a3b5e735>\u001b[0m in \u001b[0;36m<cell line: 35>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'checkpoint_iter_370000.pth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mload_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mget_rect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-1-9cef22474a2d>\u001b[0m in \u001b[0;36mget_rect\u001b[0;34m(net, images, height_size)\u001b[0m\n\u001b[1;32m     69\u001b[0m               \u001b[0mpmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid_keypoints\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m               \u001b[0mcenter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpmax\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpmin\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m               \u001b[0mradius\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.65\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpmax\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mpmin\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpmax\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mpmin\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mpose_entries\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mpose_entries\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m13\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mpose_entries\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mpose_entries\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m__former_attrs__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__former_attrs__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'testing'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'int'.\n`np.int` was a deprecated alias for the builtin `int`. To avoid this error in existing code, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations"]}]},{"cell_type":"code","metadata":{"id":"OwZu1ih8wknw"},"source":["#@title Execute to download the 3D model (.obj file) (From web image)\n","from google.colab import files\n","files.download('/content/pifuhd/results/pifuhd_final/recon/result_webImg_256.obj')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pYgb88lLy3c_"},"source":["# Image you upload"]},{"cell_type":"code","metadata":{"id":"cOWhx_lIkna_"},"source":["#@title STEP2: Testing on you images\n","#@markdown * Execute to upload your photo\n","#@markdown * And wait to see th results\n","\n","%cd '/content/sample_data'\n","#upload the image\n","from google.colab import files\n","uploaded = files.upload()\n","for fn in uploaded.keys():\n","  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n","      name=fn, length=len(uploaded[fn])))\n","\n","filename = list(uploaded.keys())[0]\n","\n","image_path = '/content/sample_data/' + filename\n","\n","#empty the folder\n","!rm -rf '/content/pifuhd/sample_images'\n","!mkdir '/content/pifuhd/sample_images'\n","\n","import cv2\n","img = cv2.imread(image_path)\n","cv2.imwrite('/content/pifuhd/sample_images/Img.png', img)\n","\n","\n","import os\n","image_path = '/content/pifuhd/sample_images/Img.png'\n","image_dir = os.path.dirname(image_path)\n","file_name = os.path.splitext(os.path.basename(image_path))[0]\n","\n","# output pathes\n","obj_path = '/content/pifuhd/results/pifuhd_final/recon/result_%s_256.obj' % file_name\n","out_img_path = '/content/pifuhd/results/pifuhd_final/recon/result_%s_256.png' % file_name\n","video_path = '/content/pifuhd/results/pifuhd_final/recon/result_%s_256.mp4' % file_name\n","video_display_path = '/content/pifuhd/results/pifuhd_final/result_%s_256_display.mp4' % file_name\n","\n","#preprosessing\n","%cd /content/lightweight-human-pose-estimation.pytorch/\n","\n","net = PoseEstimationWithMobileNet()\n","checkpoint = torch.load('checkpoint_iter_370000.pth', map_location='cpu')\n","load_state(net, checkpoint)\n","get_rect(net.cuda(), [image_path], 512)\n","\n","\n","#run\n","%cd /content/pifuhd/\n","# Warning: all images with the corresponding rectangle files under -i will be processed.\n","!python -m apps.simple_test -r 256 --use_rect -i $image_dir\n","# seems that 256 is the maximum resolution that can fit into Google Colab.\n","# If you want to reconstruct a higher-resolution mesh, please try with your own machine.\n","\n","#clear everything\n","clear_output()\n","#render video\n","from lib.colab_util import generate_video_from_obj, set_renderer, video\n","\n","renderer = set_renderer()\n","generate_video_from_obj(obj_path, out_img_path, video_path, renderer)\n","\n","# we cannot play a mp4 video generated by cv2\n","!ffmpeg -i $video_path -vcodec libx264 $video_display_path -y -loglevel quiet\n","video(video_display_path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RxlEpL4by7Xp"},"source":["#@title Execute to download the 3D model (.obj file) (From your Image)\n","from google.colab import files\n","files.download('/content/pifuhd/results/pifuhd_final/recon/result_Img_256.obj')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-Wx0eV0b1-Bc"},"source":["# Tips and Applications"]},{"cell_type":"markdown","metadata":{"id":"eUEXAvcvkVYV"},"source":["## Tips for Inputs: My results are broken!\n","\n","(Kudos to those who share results on twitter with [#pifuhd](https://twitter.com/search?q=%23pifuhd&src=recent_search_click&f=live) tag!!!!)\n","\n","Due to the limited variation in the training data, your results might be broken sometimes. Here I share some useful tips to get resonable results.\n","\n","*   Use high-res image. The model is trained with 1024x1024 images. Use at least 512x512 with fine-details. Low-res images and JPEG artifacts may result in unsatisfactory results.\n","*   Use an image with a single person. If the image contain multiple people, reconstruction quality is likely degraded.\n","*   Front facing with standing works best (or with fashion pose)\n","*   The entire body is covered within the image. (Note: now missing legs is partially supported)\n","*   Make sure the input image is well lit. Exteremy dark or bright image and strong shadow often create artifacts.\n","*   I recommend nearly parallel camera angle to the ground. High camera height may result in distorted legs or high heels.\n","*   If the background is cluttered, use less complex background or try removing it using https://www.remove.bg/ before processing.\n","*   It's trained with human only. Anime characters may not work well (To my surprise, indeed many people tried it!!).\n","*   Search on twitter with [#pifuhd](https://twitter.com/search?q=%23pifuhd&src=recent_search_click&f=live) tag to get a better sense of what succeeds and what fails.\n"]},{"cell_type":"markdown","metadata":{"id":"2d-1pR8UR7PR"},"source":["## Cool Applications\n","Special thanks to those who play with PIFuHD and came up with many creative applications!! If you made any cool applications, please tweet your demo with [#pifuhd](https://twitter.com/search?q=%23pifuhd&src=recent_search_click&f=live). I'm constantly checking results there.\n","If you need complete texture on the mesh, please try my previous work [PIFu](https://github.com/shunsukesaito/PIFu) as well! It supports 3D reconstruction + texturing from a single image although the geometry quality may not be as good as PIFuHD."]},{"cell_type":"code","metadata":{"id":"68JDAYJFSFMV","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1592786506691,"user_tz":-540,"elapsed":1452,"user":{"displayName":"Shunsuke Saito","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgjRdEU7uvjC7jQl8KzsEC_J-vZx7aCl-esOpki=s64","userId":"17785988887781081181"}},"outputId":"77edbec3-987a-4feb-bc0a-edb1aab173b6"},"source":["IPython.display.HTML('<h2>Rigging (Mixamo) + Photoreal Rendering (Blender)</h2><blockquote class=\"twitter-tweet\"><p lang=\"pt\" dir=\"ltr\">vcs ainda tem a PACHORRA de me dizer que eu não sei dançar<a href=\"https://twitter.com/hashtag/b3d?src=hash&amp;ref_src=twsrc%5Etfw\">#b3d</a> <a href=\"https://twitter.com/hashtag/pifuhd?src=hash&amp;ref_src=twsrc%5Etfw\">#pifuhd</a> <a href=\"https://t.co/kHCnLh6zxH\">pic.twitter.com/kHCnLh6zxH</a></p>&mdash; lukas arendero (@lukazvd) <a href=\"https://twitter.com/lukazvd/status/1274810484798128131?ref_src=twsrc%5Etfw\">June 21, 2020</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script><h2>FaceApp + Rigging (Mixamo)</h2><blockquote class=\"twitter-tweet\"><p lang=\"ja\" dir=\"ltr\">カツラかぶってる自分に見える <a href=\"https://twitter.com/hashtag/pifuhd?src=hash&amp;ref_src=twsrc%5Etfw\">#pifuhd</a> <a href=\"https://t.co/V8o7VduTiG\">pic.twitter.com/V8o7VduTiG</a></p>&mdash; Shuhei Tsuchida (@shuhei2306) <a href=\"https://twitter.com/shuhei2306/status/1274507242910314498?ref_src=twsrc%5Etfw\">June 21, 2020</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script><h2>Rigging (Mixamo) + AR (Adobe Aero)</AR><blockquote class=\"twitter-tweet\"><p lang=\"ja\" dir=\"ltr\">写真→PIFuHD→Mixamo→AdobeAeroでサウンド付きARを作成。Zip化してLINEでARコンテンツを共有。<br>写真が1枚あれば簡単にARの3Dアニメーションが作れる時代…凄い。<a href=\"https://twitter.com/hashtag/PIFuHD?src=hash&amp;ref_src=twsrc%5Etfw\">#PIFuHD</a> <a href=\"https://twitter.com/hashtag/AdobeAero?src=hash&amp;ref_src=twsrc%5Etfw\">#AdobeAero</a> <a href=\"https://twitter.com/hashtag/Mixamo?src=hash&amp;ref_src=twsrc%5Etfw\">#Mixamo</a> <a href=\"https://t.co/CbiMi4gZ0K\">pic.twitter.com/CbiMi4gZ0K</a></p>&mdash; モジョン (@mojon1) <a href=\"https://twitter.com/mojon1/status/1273217947872317441?ref_src=twsrc%5Etfw\">June 17, 2020</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script><h2>3D Printing</h2><blockquote class=\"twitter-tweet\"><p lang=\"ja\" dir=\"ltr\"><a href=\"https://twitter.com/hashtag/pifuhd?src=hash&amp;ref_src=twsrc%5Etfw\">#pifuhd</a> 楽しい〜<br>小さい自分プリントした <a href=\"https://t.co/4qyWuij0Hs\">pic.twitter.com/4qyWuij0Hs</a></p>&mdash; isb (@vxzxzxzxv) <a href=\"https://twitter.com/vxzxzxzxv/status/1273136266406694913?ref_src=twsrc%5Etfw\">June 17, 2020</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<h2>Rigging (Mixamo) + Photoreal Rendering (Blender)</h2><blockquote class=\"twitter-tweet\"><p lang=\"pt\" dir=\"ltr\">vcs ainda tem a PACHORRA de me dizer que eu não sei dançar<a href=\"https://twitter.com/hashtag/b3d?src=hash&amp;ref_src=twsrc%5Etfw\">#b3d</a> <a href=\"https://twitter.com/hashtag/pifuhd?src=hash&amp;ref_src=twsrc%5Etfw\">#pifuhd</a> <a href=\"https://t.co/kHCnLh6zxH\">pic.twitter.com/kHCnLh6zxH</a></p>&mdash; lukas arendero (@lukazvd) <a href=\"https://twitter.com/lukazvd/status/1274810484798128131?ref_src=twsrc%5Etfw\">June 21, 2020</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script><h2>FaceApp + Rigging (Mixamo)</h2><blockquote class=\"twitter-tweet\"><p lang=\"ja\" dir=\"ltr\">カツラかぶってる自分に見える <a href=\"https://twitter.com/hashtag/pifuhd?src=hash&amp;ref_src=twsrc%5Etfw\">#pifuhd</a> <a href=\"https://t.co/V8o7VduTiG\">pic.twitter.com/V8o7VduTiG</a></p>&mdash; Shuhei Tsuchida (@shuhei2306) <a href=\"https://twitter.com/shuhei2306/status/1274507242910314498?ref_src=twsrc%5Etfw\">June 21, 2020</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script><h2>Rigging (Mixamo) + AR (Adobe Aero)</AR><blockquote class=\"twitter-tweet\"><p lang=\"ja\" dir=\"ltr\">写真→PIFuHD→Mixamo→AdobeAeroでサウンド付きARを作成。Zip化してLINEでARコンテンツを共有。<br>写真が1枚あれば簡単にARの3Dアニメーションが作れる時代…凄い。<a href=\"https://twitter.com/hashtag/PIFuHD?src=hash&amp;ref_src=twsrc%5Etfw\">#PIFuHD</a> <a href=\"https://twitter.com/hashtag/AdobeAero?src=hash&amp;ref_src=twsrc%5Etfw\">#AdobeAero</a> <a href=\"https://twitter.com/hashtag/Mixamo?src=hash&amp;ref_src=twsrc%5Etfw\">#Mixamo</a> <a href=\"https://t.co/CbiMi4gZ0K\">pic.twitter.com/CbiMi4gZ0K</a></p>&mdash; モジョン (@mojon1) <a href=\"https://twitter.com/mojon1/status/1273217947872317441?ref_src=twsrc%5Etfw\">June 17, 2020</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script><h2>3D Printing</h2><blockquote class=\"twitter-tweet\"><p lang=\"ja\" dir=\"ltr\"><a href=\"https://twitter.com/hashtag/pifuhd?src=hash&amp;ref_src=twsrc%5Etfw\">#pifuhd</a> 楽しい〜<br>小さい自分プリントした <a href=\"https://t.co/4qyWuij0Hs\">pic.twitter.com/4qyWuij0Hs</a></p>&mdash; isb (@vxzxzxzxv) <a href=\"https://twitter.com/vxzxzxzxv/status/1273136266406694913?ref_src=twsrc%5Etfw\">June 17, 2020</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"lX5CTTW_KWhQ"},"source":[],"execution_count":null,"outputs":[]}]}